{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ee9edc-7642-45ed-8c3c-e429556db2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297dc5a6-10a6-48c0-abbc-271058e152d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Notebook variables - inherit from pipeline or job\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee917c6-4feb-4aa2-8dfb-df952608af47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a931cd1-da1d-449d-ba5a-8863d6af3a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Customer\n",
    "# Create initial customer silver table, then merge into if exists\n",
    "\n",
    "df_customer_bronze_pyspark = spark.read.table(f\"{catalog}.{schema}.customer_bronze_pyspark\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.customers_silver_pyspark\"):\n",
    "     (df_customer_bronze_pyspark\n",
    "      .select(\n",
    "          \"customer_id\",\n",
    "          \"name\",\n",
    "          \"email\",\n",
    "          \"address\",\n",
    "          \"city\",\n",
    "          \"state\",\n",
    "          \"zip_code\",\n",
    "          F.col(\"processing_time\").alias(\"last_updated\")\n",
    "      )\n",
    "      .filter(F.col(\"file_name\") == \"00.json\")\n",
    "      .write.saveAsTable(f\"{catalog}.{schema}.customers_silver_pyspark\"))\n",
    "     print(\"Created customer silver table\")\n",
    "\n",
    "# looping through bronze table by file then merging each file separately into the silver table to avoid duplicate customer ids\n",
    "for file in spark.read.table(f\"{catalog}.{schema}.customer_bronze_pyspark\").select(\"file_name\").distinct().collect():\n",
    "    #print(file[0])\n",
    "\n",
    "    #creating temp table, not actually a view, for all the individual file loads in order to merge without duplicate customer ids\n",
    "    df_customer_bronze_pyspark.filter(F.col(\"file_name\") == file[0]).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.temp_vw_bronze_customer_merge\")\n",
    "\n",
    "    #display(spark.read.table(f\"{catalog}.{schema}.temp_vw_bronze_customer_merge\"))\n",
    "    \n",
    "    spark.sql(f'''MERGE INTO {catalog}.{schema}.customers_silver_pyspark target\n",
    "        USING {catalog}.{schema}.temp_vw_bronze_customer_merge source\n",
    "        ON target.customer_id = source.customer_id\n",
    "        WHEN MATCHED AND source.operation = 'DELETE' THEN\n",
    "        DELETE\n",
    "        WHEN MATCHED AND source.operation = 'UPDATE' THEN\n",
    "        UPDATE SET\n",
    "        target.name = source.name,\n",
    "        target.email = source.email,\n",
    "        target.address = source.address,\n",
    "        target.city = source.city,\n",
    "        target.state = source.state,\n",
    "        target.zip_code = source.zip_code,\n",
    "        target.last_updated = to_timestamp(source.timestamp)\n",
    "        WHEN NOT MATCHED AND source.operation = 'NEW' THEN\n",
    "        INSERT (\n",
    "            target.customer_id,\n",
    "            target.name,\n",
    "            target.email,\n",
    "            target.address,\n",
    "            target.city,\n",
    "            target.state,\n",
    "            target.zip_code,\n",
    "            target.last_updated\n",
    "        ) VALUES (\n",
    "            source.customer_id,\n",
    "            source.name,\n",
    "            source.email,\n",
    "            source.address,\n",
    "            source.city,\n",
    "            source.state,\n",
    "            source.zip_code,\n",
    "            to_timestamp(source.timestamp)\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "# Drop temp table\n",
    "spark.sql(f'''DROP TABLE IF EXISTS {catalog}.{schema}.temp_vw_bronze_customer_merge''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6d781b-efef-4454-999a-6ca7bf8ea94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26a5665-ceb7-4a08-867d-69a347c8fe27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silver orders table\n",
    "df_orders_silver_pyspark = (spark.read.table(f\"{catalog}.{schema}.orders_bronze_pyspark\")\n",
    "                               .select(\n",
    "                                 \"order_id\",\n",
    "                                 \"customer_id\",\n",
    "                                 F.col(\"order_timestamp\").cast(\"timestamp\").alias(\"order_timestamp\"),\n",
    "                                 \"notifications\"\n",
    "                               )\n",
    ")\n",
    "\n",
    "df_orders_silver_pyspark.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.orders_silver_pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed13d7e-f43e-4337-8737-666e9b524e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b818b916-d845-49ac-84fb-262aba1bb640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silver status table\n",
    "df_status_silver_pyspark = (spark.read.table(f\"{catalog}.{schema}.status_bronze_pyspark\")\n",
    "                               .select(\n",
    "                                 \"order_id\",\n",
    "                                 \"order_status\",\n",
    "                                 F.col(\"status_timestamp\").cast(\"timestamp\").alias(\"status_timestamp\")\n",
    "                               )\n",
    ")\n",
    "\n",
    "df_status_silver_pyspark.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.status_silver_pyspark\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Silver",
   "widgets": {
    "catalog": {
     "currentValue": "demo",
     "nuid": "723561b8-38a8-4e9f-a68a-7f45ced2a79c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "sim_retail_demo",
     "nuid": "cb2f5966-d078-41e6-8aaf-1adb2fe3661c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sim_retail_demo",
      "label": "",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sim_retail_demo",
      "label": "",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
